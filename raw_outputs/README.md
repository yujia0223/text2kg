## Raw Outputs

Contained here are some of the raw outputs of benchmarking from our models. These files can be used to start evaluation directly without having to rerun any benchmarking. To do this, supply the benchmark_and_evaluate.py file with:

```--pickle=PATH_TO_PICKLE_FILE```

Additional model output files that are not uploaded here can be found at [UofA-LINGO/model-benchmark-outputs](https://huggingface.co/datasets/UofA-LINGO/model-benchmark-outputs). Please note, this set of outputs does not contain all benchmarking outputs we have created, but does have all output files for the results we are reporting on.